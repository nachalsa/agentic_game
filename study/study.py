import os
import time
import logging
import requests
from datetime import datetime
from contextlib import contextmanager
import litellm
from crewai import Agent, Task, Crew, Process
from crewai.tools import tool
from dotenv import load_dotenv
import json
import re

# Web scraping libraries
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'crewai_simple_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í¬ë£¨ AI í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
os.environ.setdefault("OTEL_SDK_DISABLED", "true")
os.environ.setdefault("CREWAI_DISABLE_TELEMETRY", "true")

# í™˜ê²½ ë³€ìˆ˜
MODEL_NAME = os.getenv("DEFAULT_LLM", "mistral")
API_BASE_URL = os.getenv("DEFAULT_URL", "http://192.168.100.26:11434")
API_KEY = os.getenv("DEFAULT_API_KEY", "ollama")
TIMEOUT = int(os.getenv("TIMEOUT", "30"))
MAX_EXECUTION_TIME = int(os.getenv("MAX_EXECUTION_TIME", "600"))

# URL ì •ê·œí™”
if not API_BASE_URL.endswith('/v1'):
    API_BASE_URL = API_BASE_URL.rstrip('/') + '/v1'

# ===== êµ¬ê¸€ ì§ì ‘ ê²€ìƒ‰ ë„êµ¬ =====

# User-Agent ë¦¬ìŠ¤íŠ¸ (ì°¨ë‹¨ ë°©ì§€)
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def safe_delay():
    """ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ëœë¤ ë”œë ˆì´"""
    delay = random.uniform(2, 5)
    time.sleep(delay)
    logger.info(f"ëŒ€ê¸° ì¤‘... ({delay:.1f}ì´ˆ)")

def get_random_headers():
    """ëœë¤ í—¤ë” ìƒì„±"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

def extract_text_from_url(url, max_chars=1500):
    """URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        logger.info(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {url}")
        headers = get_random_headers()
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator=' ', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        clean_text = ' '.join(lines)
        
        # ê¸¸ì´ ì œí•œ
        if len(clean_text) > max_chars:
            clean_text = clean_text[:max_chars] + "..."
        
        return clean_text
        
    except Exception as e:
        logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
        return ""

def parse_google_results(html_content):
    """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë§í¬ë“¤ ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, 'html.parser')
    results = []
    
    # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ì°¾ê¸°
    for g in soup.find_all('div', class_='g'):
        try:
            # ì œëª©
            title_elem = g.find('h3')
            title = title_elem.get_text() if title_elem else "ì œëª© ì—†ìŒ"
            
            # URL
            link_elem = g.find('a')
            if link_elem and 'href' in link_elem.attrs:
                url = link_elem['href']
                
                # êµ¬ê¸€ ë‚´ë¶€ ë§í¬ ì œì™¸
                if url.startswith('http') and 'google.com' not in url:
                    results.append({
                        'title': title,
                        'url': url
                    })
                    
        except Exception as e:
            logger.error(f"ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue
    
    return results[:5]  # ìƒìœ„ 5ê°œë§Œ

@tool("Google Search")
def google_search(query: str) -> str:
    """
    êµ¬ê¸€ ì§ì ‘ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  í‚¤ì›Œë“œ (ì˜ì–´ ê¶Œì¥)
    
    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë° ë³¸ë¬¸ ë‚´ìš©
    """
    try:
        logger.info(f"êµ¬ê¸€ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # êµ¬ê¸€ ê²€ìƒ‰ URL
        search_url = f"https://www.google.com/search?q={query}&num=10"
        
        # ê²€ìƒ‰ ìš”ì²­
        headers = get_random_headers()
        response = requests.get(search_url, headers=headers, timeout=15)
        
        if response.status_code == 429:
            logger.warning("ìš”ì²­ ì œí•œ ê°ì§€, ê¸´ ëŒ€ê¸° í›„ ì¬ì‹œë„")
            time.sleep(30)
            response = requests.get(search_url, headers=headers, timeout=15)
        
        response.raise_for_status()
        
        # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        results = parse_google_results(response.text)
        
        if not results:
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê²°ê³¼ í¬ë§·íŒ… ì‹œì‘
        formatted_results = f"ğŸ” '{query}' êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼:\n\n"
        
        # ê° ê²°ê³¼ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
        for i, result in enumerate(results, 1):
            title = result['title']
            url = result['url']
            
            # ë”œë ˆì´ (ì²« ë²ˆì§¸ ì œì™¸)
            if i > 1:
                safe_delay()
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content = extract_text_from_url(url)
            
            if content:
                formatted_results += f"{i}. **{title}**\n"
                formatted_results += f"   ğŸ“„ {content[:300]}{'...' if len(content) > 300 else ''}\n"
                formatted_results += f"   ğŸ”— {url}\n\n"
            else:
                # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨
                formatted_results += f"{i}. **{title}**\n"
                formatted_results += f"   ğŸ”— {url}\n\n"
        
        logger.info(f"êµ¬ê¸€ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ì²˜ë¦¬")
        return formatted_results
        
    except requests.exceptions.RequestException as e:
        error_msg = f"êµ¬ê¸€ ê²€ìƒ‰ ì—°ê²° ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        return error_msg
    except Exception as e:
        error_msg = f"êµ¬ê¸€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        return error_msg

# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====
def retry_with_backoff(func, max_retries=3, base_delay=1):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            delay = base_delay * (2 ** attempt)
            logger.warning(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}. {delay}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(delay)

def test_litellm_connection():
    def _test():
        logger.info("LiteLLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        response = litellm.completion(
            model=f"openai/{MODEL_NAME}",
            messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}],
            api_base=API_BASE_URL,
            api_key=API_KEY,
            temperature=0.7,
            max_tokens=100,
            timeout=TIMEOUT,
            drop_params=True
        )
        logger.info("LiteLLM í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        return True
    return retry_with_backoff(_test)

def test_google_search():
    try:
        logger.info("êµ¬ê¸€ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_result = google_search.func("AI trends 2025")
        if "êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼" in test_result or "ì˜¤ë¥˜" in test_result:
            logger.info("âœ… êµ¬ê¸€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
        else:
            logger.warning("âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤")
            return False
    except Exception as e:
        logger.error(f"âŒ êµ¬ê¸€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def setup_litellm_global_config():
    litellm.api_base = API_BASE_URL
    litellm.api_key = API_KEY
    litellm.drop_params = True
    logger.info("LiteLLM ì „ì—­ ì„¤ì • ì™„ë£Œ")

# ===== 3ë‹¨ê³„ ì—ì´ì „íŠ¸ ìƒì„± =====
def create_simple_agents():
    
    # 1ë‹¨ê³„: ê²€ìƒ‰ ì „ëµê°€
    search_strategist = Agent(
        role='ê²€ìƒ‰ ì „ëµê°€',
        goal='ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ íš¨ê³¼ì ì¸ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ì„ ìƒì„±',
        backstory='''ë‹¹ì‹ ì€ ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ í¬ê´„ì ì¸ ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•œ 
        ê²€ìƒ‰ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ìˆ , ë¹„ì¦ˆë‹ˆìŠ¤, ê·œì œ, ë¯¸ë˜ì „ë§ ë“± 
        ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.''',
        verbose=True,
        allow_delegation=False,
        llm=f"openai/{MODEL_NAME}",
        max_tokens=800,
        temperature=0.6
    )

    # 2ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ê°€  
    information_gatherer = Agent(
        role='ì •ë³´ ìˆ˜ì§‘ê°€',
        goal='ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  í•œêµ­ì–´ë¡œ ì •ë¦¬',
        backstory='''ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ 
        ìˆ˜ì§‘í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  
        í•œêµ­ì–´ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.''',
        verbose=True,
        allow_delegation=False,
        tools=[google_search],
        llm=f"openai/{MODEL_NAME}",
        max_tokens=2000,
        temperature=0.5
    )

    # 3ë‹¨ê³„: ì½˜í…ì¸  ì‘ì„±ê°€
    content_creator = Agent(
        role='ì½˜í…ì¸  ì‘ì„±ê°€',
        goal='ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ í’ˆì§ˆ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì‘ì„±',
        backstory='''ë‹¹ì‹ ì€ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë…ìê°€ í¥ë¯¸ë¡­ê²Œ ì½ì„ ìˆ˜ ìˆëŠ” 
        ê³ í’ˆì§ˆ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìµœì‹  ì •ë³´ì™€ ì‹¤ìš©ì ì¸ 
        ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ë…ìì—ê²Œ ê°€ì¹˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.''',
        verbose=True,
        allow_delegation=False,
        llm=f"openai/{MODEL_NAME}",
        max_tokens=3000,
        temperature=0.7
    )
    
    return search_strategist, information_gatherer, content_creator

# ===== 3ë‹¨ê³„ ì‘ì—… ìƒì„± =====
def create_simple_tasks(search_strategist, information_gatherer, content_creator, research_topic):
    
    # 1ë‹¨ê³„: ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
    strategy_task = Task(
        description=f'''ì£¼ì œ: "{research_topic}"

ì´ ì£¼ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ì—°êµ¬ë¥¼ ìœ„í•´ íš¨ê³¼ì ì¸ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ë‹¤ìŒ ê´€ì ë“¤ì„ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë§Œë“œì„¸ìš”:
1. ìµœì‹  ê¸°ìˆ  ë°œì „ì‚¬í•­ (latest technology, breakthrough, innovation)
2. ë¹„ì¦ˆë‹ˆìŠ¤ ë™í–¥ (business trends, market adoption, industry impact)  
3. ê·œì œ ë° ì •ì±… (regulation, policy, governance, ethics)
4. ë¯¸ë˜ ì „ë§ (future outlook, predictions, forecasts)
5. ì‹¤ì œ ì‚¬ë¡€ (case studies, real-world applications)

ê° ê´€ì ë³„ë¡œ 1-2ê°œì”©, ì´ 6-8ê°œì˜ êµ¬ì²´ì ì¸ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ëª¨ë“  ì¿¼ë¦¬ëŠ” ë°˜ë“œì‹œ ì˜ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹:
1. AI breakthrough technology 2025
2. generative AI business adoption trends
3. AI regulation policy developments 2025
...''',
        
        expected_output='''6-8ê°œì˜ êµ¬ì²´ì ì¸ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸.
ê° ì¿¼ë¦¬ëŠ” ë²ˆí˜¸ì™€ í•¨ê»˜ ëª…í™•í•˜ê²Œ ë‚˜ì—´ë˜ì–´ì•¼ í•¨.
ëª¨ë“  ì¿¼ë¦¬ëŠ” ì˜ì–´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•¨.''',
        
        agent=search_strategist
    )
    
    # 2ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘
    research_task = Task(
        description='''1ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

ìˆ˜í–‰ ë°©ë²•:
1. ì œê³µëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ í•˜ë‚˜ì”© Web Search ë„êµ¬ë¡œ ê²€ìƒ‰
2. ê° ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
3. ê´€ë ¨ì„± ë†’ì€ ì¶”ê°€ ì¿¼ë¦¬ê°€ ìˆë‹¤ë©´ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
4. ì´ 6-8íšŒì˜ ê²€ìƒ‰ì„ í†µí•´ ë‹¤ì–‘í•œ ì •ë³´ ìˆ˜ì§‘

ì£¼ì˜ì‚¬í•­:
        - ë°˜ë“œì‹œ Google Search ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
- ê° ê²€ìƒ‰ í›„ ê²°ê³¼ë¥¼ ê°„ë‹¨íˆ ìš”ì•½
- ìµœì‹  ì •ë³´ì™€ êµ¬ì²´ì ì¸ ë°ì´í„°ì— ì§‘ì¤‘
- ëª¨ë“  ì •ë¦¬ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±

ê²€ìƒ‰ì´ ì™„ë£Œë˜ë©´ ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.''',
        
        expected_output='''ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìˆ˜ì§‘ëœ ìµœì‹  ì •ë³´ì˜ ì¢…í•© ë³´ê³ ì„œ (í•œêµ­ì–´).
ê° ê²€ìƒ‰ ê²°ê³¼ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¶œì²˜ë¥¼ í¬í•¨.
600-800ë‹¨ì–´ ë¶„ëŸ‰ì˜ ì²´ê³„ì ì¸ ì •ë³´ ì •ë¦¬.''',
        
        agent=information_gatherer,
        context=[strategy_task]
    )

    # 3ë‹¨ê³„: ì½˜í…ì¸  ì‘ì„±
    content_task = Task(
        description=f'''ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ "{research_topic}: í˜„ì‹¤ì´ ëœ ë¯¸ë˜ ê¸°ìˆ ë“¤" ì œëª©ì˜ 
ê³ í’ˆì§ˆ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ë¶„ëŸ‰: 900-1200ë‹¨ì–´
- ì–¸ì–´: í•œêµ­ì–´
- ì–´ì¡°: ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ
- êµ¬ì¡°: ë„ì…ë¶€ â†’ ì£¼ìš” íŠ¸ë Œë“œ ë¶„ì„ â†’ ì‹¤ìƒí™œ ì˜í–¥ â†’ ë¯¸ë˜ ì „ë§ â†’ ê²°ë¡ 

í¬í•¨ ìš”ì†Œ:
1. í¥ë¯¸ë¡œìš´ ë„ì…ë¶€ (ìµœì‹  ì‚¬ë¡€ë‚˜ ë†€ë¼ìš´ ë°œì „ì‚¬í•­)
2. ê²€ìƒ‰ì—ì„œ ë°œê²¬ëœ êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ì‚¬ë¡€
3. ë…ìì—ê²Œ ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ
4. ë¯¸ë˜ì— ëŒ€í•œ ì „ë¬¸ê°€ì  ì „ë§
5. ë…ìê°€ ì¤€ë¹„í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸

ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì—°êµ¬ì—ì„œ ìˆ˜ì§‘ëœ ìµœì‹  ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.''',
        
        expected_output='''900-1200ë‹¨ì–´ ë¶„ëŸ‰ì˜ ì™„ì„±ëœ í•œêµ­ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.
ìˆ˜ì§‘ëœ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹¤ìš©ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë‚´ìš©.
ë…ìê°€ ëê¹Œì§€ ì½ê³  ì‹¶ì–´í•˜ëŠ” í¥ë¯¸ë¡œìš´ êµ¬ì„±.''',
        
        agent=content_creator,
        context=[research_task]
    )
    
    return strategy_task, research_task, content_task

def save_result(result, research_topic):
    """ê²°ê³¼ ì €ì¥"""
    if not result:
        logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    safe_topic = re.sub(r'[^\w\s-]', '', research_topic.replace(' ', '_'))
    filename = f"simple_ai_blog_{safe_topic}_{timestamp}.md"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# {research_topic}\n")
            f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"AI ëª¨ë¸: {MODEL_NAME}\n\n")
            f.write("---\n\n")
            f.write(str(result))
        
        logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename
    except Exception as e:
        logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def run_simple_crew(research_topic="2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ"):
    """ê°„ë‹¨í•œ 3ë‹¨ê³„ í¬ë£¨ ì‹¤í–‰"""
    try:
        logger.info("=" * 60)
        logger.info("ğŸš€ ê°„ë‹¨í•œ CrewAI ì‹œìŠ¤í…œ ì‹œì‘")
        logger.info(f"ğŸ“‹ ì—°êµ¬ ì£¼ì œ: {research_topic}")
        logger.info("=" * 60)
        
        # ì„¤ì • ë° ì—°ê²° í…ŒìŠ¤íŠ¸
        logger.info(f"ğŸ”§ ì„¤ì •: {MODEL_NAME} @ {API_BASE_URL}")
        test_litellm_connection()
        test_google_search()
        setup_litellm_global_config()
        
        # ì—ì´ì „íŠ¸ ë° ì‘ì—… ìƒì„±
        logger.info("\nğŸ¤– 3ë‹¨ê³„ AI íŒ€ êµ¬ì„± ì¤‘...")
        search_strategist, information_gatherer, content_creator = create_simple_agents()
        strategy_task, research_task, content_task = create_simple_tasks(
            search_strategist, information_gatherer, content_creator, research_topic
        )
        
        # í¬ë£¨ ìƒì„± ë° ì‹¤í–‰
        simple_crew = Crew(
            agents=[search_strategist, information_gatherer, content_creator],
            tasks=[strategy_task, research_task, content_task],
            process=Process.sequential,
            verbose=True,
            max_execution_time=MAX_EXECUTION_TIME,
            memory=False,
            max_rpm=30  # RPM ì¦ê°€
        )
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ¯ 3ë‹¨ê³„ AI íŒ€ ì‘ì—… ì‹œì‘")
        logger.info("1ï¸âƒ£ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½ â†’ 2ï¸âƒ£ ì •ë³´ ìˆ˜ì§‘ â†’ 3ï¸âƒ£ ì½˜í…ì¸  ì‘ì„±")
        logger.info("=" * 60)
        
        result = simple_crew.kickoff()
        
        # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
        saved_file = save_result(result, research_topic)
        
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ‰ ì‘ì—… ì™„ë£Œ!")
        logger.info("=" * 60)
        print(f"\nğŸ“„ ìƒì„±ëœ ì½˜í…ì¸ :")
        print("=" * 80)
        print(result)
        print("=" * 80)
        
        if saved_file:
            logger.info(f"\nğŸ“ ê²°ê³¼ê°€ '{saved_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ í¬ë£¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return None

# ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # ì‚¬ìš©ì ì •ì˜ ì£¼ì œ
    custom_topic = "2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ"
    
    print(f"ğŸ¯ ì—°êµ¬ ì£¼ì œ: {custom_topic}")
    print("ğŸ¤– ê°„ë‹¨í•œ 3ë‹¨ê³„ AI íŒ€ì´ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    result = run_simple_crew(custom_topic)
    
    if result:
        print(f"\nâœ… '{custom_topic}' ì—°êµ¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì‘ì—… ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")