import os
import logging
from datetime import datetime
import litellm
from crewai import Agent, Task, Crew, Process
from crewai.tools import tool
from dotenv import load_dotenv
import argparse
import re
import requests
from urllib.parse import urljoin, urlparse
import time

from ddgs import DDGS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'research_crew_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ì„¤ì •
load_dotenv()
os.environ.setdefault("OTEL_SDK_DISABLED", "true")
os.environ.setdefault("CREWAI_DISABLE_TELEMETRY", "true")

# ì„¤ì • í´ë˜ìŠ¤
class ResearchConfig:
    """ë¦¬ì„œì¹˜ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 topic: str,
                 search_queries_count: int = 5,
                 max_pages_per_query: int = 3,
                 word_count_range: tuple = (700, 900),
                 language: str = "í•œêµ­ì–´",
                 report_type: str = "ë¸”ë¡œê·¸"):
        self.topic = topic
        self.search_queries_count = search_queries_count
        self.max_pages_per_query = max_pages_per_query
        self.word_count_range = word_count_range
        self.language = language
        self.report_type = report_type
        
        # íŒŒì¼ëª…ìš© ì•ˆì „í•œ í† í”½ëª… ìƒì„±
        self.safe_topic = re.sub(r'[^\w\s-]', '', topic.replace(' ', '_'))[:50]

# í™˜ê²½ ë³€ìˆ˜
MODEL_NAME = os.getenv("DEFAULT_LLM", "cpatonn/Devstral-Small-2507-AWQ")
API_BASE_URL = os.getenv("DEFAULT_URL", "http://localhost:54321")
API_KEY = os.getenv("DEFAULT_API_KEY", "huntr/x_How_It's_Done")
TIMEOUT = int(os.getenv("TIMEOUT", "30"))
MAX_EXECUTION_TIME = int(os.getenv("MAX_EXECUTION_TIME", "900"))

if not API_BASE_URL.endswith('/v1'):
    API_BASE_URL = API_BASE_URL.rstrip('/') + '/v1'

# í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬
@tool("Web Search Tool")
def web_search_tool(query: str) -> str:
    """ì›¹ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ì „ì²´ í˜ì´ì§€ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” í†µí•© ë„êµ¬"""
    try:
        logger.info(f"ğŸ” í†µí•© ì›¹ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # 1ë‹¨ê³„: ì›¹ ê²€ìƒ‰
        ddgs = DDGS()
        search_results = ddgs.text(query=query, region='wt-wt', safesearch='moderate', max_results=5)
        
        if not search_results:
            logger.warning(f"âš ï¸ '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¤‘ë³µ URL ì œê±°
        unique_urls = []
        seen_urls = set()
        
        for result in search_results:
            url = result.get('href', '')
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            
            if url and url not in seen_urls:
                unique_urls.append({'url': url, 'title': title})
                seen_urls.add(url)
        
        if not unique_urls:
            return f"'{query}'ì— ëŒ€í•œ ìœ íš¨í•œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 2ë‹¨ê³„: í˜ì´ì§€ í¬ë¡¤ë§ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_contents = []
        max_pages = min(3, len(unique_urls))  # ìµœëŒ€ 3ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬
        
        for i, item in enumerate(unique_urls[:max_pages]):
            url = item['url']
            title = item['title']
            
            try:
                logger.info(f"ğŸ“„ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ({i+1}/{max_pages}): {url}")
                
                # í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                
                # 3ë‹¨ê³„: trafilaturaë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                try:
                    import trafilatura
                    
                    # HTMLì—ì„œ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    extracted_text = trafilatura.extract(
                        response.text,
                        include_comments=False,
                        include_tables=True,
                        include_images=False,
                        output_format='text'
                    )
                    
                    if extracted_text and len(extracted_text.strip()) > 100:
                        # í…ìŠ¤íŠ¸ ì •ì œ
                        clean_text = extracted_text.strip()
                        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì œí•œ
                        clean_text = re.sub(r'\n{3,}', '\n\n', clean_text)
                        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²« 2000ìë§Œ ì‚¬ìš©
                        if len(clean_text) > 2000:
                            clean_text = clean_text[:2000] + "..."
                        
                        extracted_contents.append({
                            'title': title,
                            'url': url,
                            'content': clean_text
                        })
                        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(clean_text)}ì")
                    else:
                        logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë‚´ìš© ë¶€ì¡±: {url}")
                        
                except ImportError:
                    logger.error("âŒ trafilatura ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                    # ê¸°ë³¸ HTML íƒœê·¸ ì œê±° ë°©ì‹ìœ¼ë¡œ í´ë°±
                    clean_text = re.sub(r'<[^>]+>', '', response.text)
                    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
                    if len(clean_text) > 500:
                        extracted_contents.append({
                            'title': title,
                            'url': url,
                            'content': clean_text[:1000] + "..."
                        })
                
                # ìš”ì²­ ê°„ ì§€ì—°
                time.sleep(1)
                
            except requests.RequestException as e:
                logger.warning(f"âš ï¸ í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url}: {str(e)}")
                continue
            except Exception as e:
                logger.warning(f"âš ï¸ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜ {url}: {str(e)}")
                continue
        
        # 4ë‹¨ê³„: ê²°ê³¼ í¬ë§·íŒ…
        if not extracted_contents:
            return f"'{query}' ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
        
        formatted_result = f"ğŸ” '{query}' ê²€ìƒ‰ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼:\n\n"
        
        for i, content in enumerate(extracted_contents, 1):
            formatted_result += f"ğŸ“„ {i}. {content['title']}\n"
            formatted_result += f"ğŸ”— ì¶œì²˜: {content['url']}\n"
            formatted_result += f"ğŸ“ ë‚´ìš©:\n{content['content']}\n"
            formatted_result += "-" * 80 + "\n\n"
        
        logger.info(f"âœ… í†µí•© ê²€ìƒ‰ ì™„ë£Œ: {len(extracted_contents)}ê°œ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        return formatted_result
        
    except Exception as e:
        error_msg = f"âŒ í†µí•© ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        return error_msg

# ì£¼ì œë³„ í”„ë¦¬ì…‹ (ë™ì¼)
RESEARCH_PRESETS = {
    "ai": "2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ",
    "blockchain": "2025ë…„ ë¸”ë¡ì²´ì¸ ê¸°ìˆ  ë°œì „", 
    "climate": "ì§€ì†ê°€ëŠ¥í•œ ê¸°í›„ ê¸°ìˆ  í˜ì‹ ",
    "health": "ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ê¸°ìˆ  íŠ¸ë Œë“œ",
    "fintech": "í•€í…Œí¬ ì‚°ì—… ìµœì‹  ë™í–¥",
    "architecture": "í˜„ëŒ€ ê±´ì¶• ê¸°ìˆ  í˜ì‹ ",
    "education": "êµìœ¡ ê¸°ìˆ  ë””ì§€í„¸ ì „í™˜",
    "energy": "ì¬ìƒ ì—ë„ˆì§€ ê¸°ìˆ  ë°œì „",
    "space": "ìš°ì£¼ ê¸°ìˆ  ë° íƒì‚¬ ë™í–¥",
    "food": "í‘¸ë“œí…Œí¬ ì‚°ì—… í˜ì‹ "
}

def get_preset_topic(preset_name: str) -> str:
    """í”„ë¦¬ì…‹ ì£¼ì œ ë°˜í™˜ (ì—†ìœ¼ë©´ ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜)"""
    return RESEARCH_PRESETS.get(preset_name.lower(), preset_name)

# ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ í´ë˜ìŠ¤ (ê¸°ì¡´ ì´ë¦„ ìœ ì§€)
class UniversalResearchCrew:
    """ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ë¦¬ì„œì¹˜ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” AI í¬ë£¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: ResearchConfig):
        self.config = config
        self.setup_environment()
    
    def setup_environment(self):
        """LiteLLM í™˜ê²½ ì„¤ì •"""
        litellm.api_base = API_BASE_URL
        litellm.api_key = API_KEY
        litellm.drop_params = True
        logger.info("í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    
    def create_agents(self):
        """ì—ì´ì „íŠ¸ ìƒì„± (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)"""
        
        planner = Agent(
            role='ì—°êµ¬ ê³„íš ì „ë¬¸ê°€',
            goal=f'{self.config.topic}ì— ëŒ€í•œ íš¨ê³¼ì ì¸ ì›¹ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½',
            backstory='''ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì „ëµê°€ì…ë‹ˆë‹¤. 
            ë³µì¡í•œ ì£¼ì œë¥¼ í•µì‹¬ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ê³ , ìµœì‹  ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ê²€ìƒ‰ì–´ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.''',
            verbose=True,
            allow_delegation=False,
            llm=f"openai/{MODEL_NAME}",
            max_tokens=1024,
            temperature=0.6
        )

        researcher = Agent(
            role='ì „ë¬¸ ë¦¬ì„œì¹˜ ë¶„ì„ê°€',
            goal=f'{self.config.topic}ì— ëŒ€í•œ ì¢…í•©ì ì´ê³  ì‹¬ì¸µì ì¸ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„',
            backstory='''í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ , 
            ì›¹í˜ì´ì§€ ì „ì²´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ìˆ™ë ¨ëœ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.''',
            verbose=True,
            allow_delegation=False,
            tools=[web_search_tool],
            llm=f"openai/{MODEL_NAME}",
            max_tokens=2000,
            temperature=0.7
        )

        writer = Agent(
            role='ì „ë¬¸ ì½˜í…ì¸  ì‘ê°€',
            goal=f'{self.config.topic}ì— ëŒ€í•œ ë§¤ë ¥ì ì´ê³  ìœ ìµí•œ {self.config.report_type} ì‘ì„±',
            backstory=f'''ë³µì¡í•œ ì •ë³´ë¥¼ {self.config.language}ë¡œ ëª…í™•í•˜ê³  ë§¤ë ¥ì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì „ë¬¸ ì‘ê°€ì…ë‹ˆë‹¤. 
            ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ìµœì‹  ì •ë³´ë¥¼ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ ì½˜í…ì¸ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.''',
            verbose=True,
            allow_delegation=False,
            llm=f"openai/{MODEL_NAME}",
            max_tokens=2000,
            temperature=0.8
        )
        
        return planner, researcher, writer
    
    def create_tasks(self, planner, researcher, writer):
        """íƒœìŠ¤í¬ ìƒì„± (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€, ë‚´ìš©ë§Œ ê°œì„ )"""
        
        # 1. ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ (ë™ì¼)
        planning_task = Task(
            description=f'''"{self.config.topic}"ì— ëŒ€í•œ í¬ê´„ì ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
            
            ì´ ì£¼ì œë¥¼ ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•˜ì—¬ {self.config.search_queries_count}ê°œì˜ êµ¬ì²´ì ì´ê³  íš¨ê³¼ì ì¸ ì˜ì–´ ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
            
            1. ìµœì‹  ë™í–¥ ë° ë°œì „ì‚¬í•­ (latest trends, developments, innovations)
            2. ì „ë¬¸ê°€ ë¶„ì„ ë° ì—°êµ¬ ê²°ê³¼ (expert analysis, research, studies)  
            3. ì‹¤ì œ ì ìš© ì‚¬ë¡€ ë° ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” (case studies, applications, examples)
            4. ë¯¸ë˜ ì „ë§ ë° ì˜ˆì¸¡ (future outlook, predictions, forecasts)
            5. ì‚°ì—…ë³„ ì˜í–¥ ë° í™œìš© (industry impact, implementation)
            
            **ì¤‘ìš”í•œ í˜•ì‹ ìš”êµ¬ì‚¬í•­:**
            - ê° ì¿¼ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
            - ì •í™•íˆ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
            SEARCH_QUERY_1: "ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´"
            SEARCH_QUERY_2: "ë‘ ë²ˆì§¸ ê²€ìƒ‰ì–´"  
            SEARCH_QUERY_3: "ì„¸ ë²ˆì§¸ ê²€ìƒ‰ì–´"
            SEARCH_QUERY_4: "ë„¤ ë²ˆì§¸ ê²€ìƒ‰ì–´"
            SEARCH_QUERY_5: "ë‹¤ì„¯ ë²ˆì§¸ ê²€ìƒ‰ì–´"
            
            ê° ì¿¼ë¦¬ëŠ” ëª…í™•í•˜ê³  ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•´ì•¼ í•˜ë©°, 2024-2025ë…„ ìµœì‹  ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•˜ì„¸ìš”.''',
            
            expected_output=f'''ì •í™•íˆ ë‹¤ìŒ í˜•ì‹ì˜ {self.config.search_queries_count}ê°œ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬:
            SEARCH_QUERY_1: "query1"
            SEARCH_QUERY_2: "query2"
            SEARCH_QUERY_3: "query3"
            SEARCH_QUERY_4: "query4"  
            SEARCH_QUERY_5: "query5"
            ì£¼ì œ: {self.config.topic}ì— ìµœì í™”ëœ ì„œë¡œ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë“¤''',
            agent=planner
        )
        
        # 2. ì •ë³´ ìˆ˜ì§‘ (í†µí•© ë„êµ¬ ì‚¬ìš©ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
        research_task = Task(
            description=f'''ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ì„ í™œìš©í•˜ì—¬ "{self.config.topic}"ì— ëŒ€í•œ ì‹¬ì¸µ ì›¹ ê²€ìƒ‰ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

            **í•„ìˆ˜ ìˆ˜í–‰ ì ˆì°¨:**
            1. ì´ì „ Task ê²°ê³¼ì—ì„œ "SEARCH_QUERY_1:", "SEARCH_QUERY_2:" ë“±ì˜ í˜•ì‹ìœ¼ë¡œ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ì„ ì°¾ì•„ ì¶”ì¶œí•©ë‹ˆë‹¤.
            2. ê° SEARCH_QUERY_Xì—ì„œ ë”°ì˜´í‘œ ì•ˆì˜ ê²€ìƒ‰ì–´ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            3. ì¶”ì¶œëœ **ëª¨ë“  ê²€ìƒ‰ì–´ë¥¼ í•˜ë‚˜ì”© ìˆœì„œëŒ€ë¡œ** 'Web Search Tool'ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            4. í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ê°€ ìë™ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
               - ì›¹ ê²€ìƒ‰ ì‹¤í–‰
               - ìƒìœ„ 3ê°œ í˜ì´ì§€ í¬ë¡¤ë§
               - ì „ì²´ HTMLì—ì„œ trafilaturaë¡œ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
               - ì •ì œëœ í…ìŠ¤íŠ¸ ê²°ê³¼ ë°˜í™˜
            5. ê²€ìƒ‰í•  ë•Œë§ˆë‹¤ "ğŸ” ê²€ìƒ‰ ì¤‘: X/{self.config.search_queries_count} - [ê²€ìƒ‰ì–´]" í˜•íƒœë¡œ ì§„í–‰ìƒí™©ì„ ì•Œë ¤ì£¼ì„¸ìš”.
            
            **ë³´ê³ ì„œ ì‘ì„± ìš”êµ¬ì‚¬í•­:**
            ëª¨ë“  ê²€ìƒ‰ ì™„ë£Œ í›„, ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í¬í•¨í•œ ì¢…í•© ë³´ê³ ì„œë¥¼ **ë°˜ë“œì‹œ {self.config.language}ë¡œ** ì‘ì„±í•˜ì„¸ìš”:
            - ì£¼ìš” íŠ¸ë Œë“œ ë° ë°œì „ì‚¬í•­
            - í•µì‹¬ í†µê³„ ë° ë°ì´í„°  
            - êµ¬ì²´ì  ì‚¬ë¡€ ë° ì‹¤ë¬´ ì ìš©
            - ì „ë¬¸ê°€ ì˜ê²¬ ë° ë¶„ì„
            - ë¯¸ë˜ ì „ë§
            
            **ì ˆëŒ€ì ìœ¼ë¡œ ì¤‘ìš”**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ì–´ë¡œ ë‚˜ì™€ë„ ë³´ê³ ì„œëŠ” **ë¬´ì¡°ê±´ {self.config.language}ë¡œë§Œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.''',
            
            expected_output=f'''"{self.config.topic}"ì— ëŒ€í•œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸, ìµœì‹  í†µê³„ ë° ì‹¤ì œ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ëŠ” 
            400-500ë‹¨ì–´ ë¶„ëŸ‰ì˜ ìƒì„¸í•œ ì—°êµ¬ ìš”ì•½ ë³´ê³ ì„œ (**ë°˜ë“œì‹œ {self.config.language}ë¡œ ì‘ì„±**).
            í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ë¡œ ì¶”ì¶œí•œ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±.''',
            
            agent=researcher,
            context=[planning_task]
        )

        # 3. ì½˜í…ì¸  ì‘ì„± (ë™ì¼)
        write_task = Task(
            description=f'''ì—°êµ¬ ìš”ì•½ ë³´ê³ ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ "{self.config.topic}"ì— ëŒ€í•œ 
            **ë°˜ë“œì‹œ {self.config.language}ë¡œë§Œ ì‘ì„±ëœ** {self.config.report_type}ì„ ì‘ì„±í•©ë‹ˆë‹¤.
            
            **ì ˆëŒ€ì  ì–¸ì–´ ìš”êµ¬ì‚¬í•­:**
            - **ëª¨ë“  ë‚´ìš©ì€ {self.config.language}ë¡œë§Œ ì‘ì„±**í•´ì•¼ í•©ë‹ˆë‹¤
            - ì œëª©, ì†Œì œëª©, ë³¸ë¬¸, ëª¨ë“  í…ìŠ¤íŠ¸ê°€ {self.config.language}ì—¬ì•¼ í•©ë‹ˆë‹¤
            - ì˜ì–´ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
            
            **êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­:**
            - ë¶„ëŸ‰: {self.config.word_count_range[0]}-{self.config.word_count_range[1]}ë‹¨ì–´
            - ë§¤ë ¥ì ì¸ {self.config.language} ì œëª©ê³¼ ë¶€ì œëª© ì‚¬ìš©
            - ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ {self.config.language} í‘œí˜„
            - ì—°êµ¬ ë³´ê³ ì„œì˜ ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•œ í˜„ì‹¤ì ì¸ ë‚´ìš©
            - ë…ìì˜ ê´€ì‹¬ì„ ë„ëŠ” êµ¬ì„±
            - ì‹¤ì œ ì‚¬ë¡€ë‚˜ êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨
            
            **êµ¬ì¡° ({self.config.language}ë¡œ ì‘ì„±):**
            1. í¥ë¯¸ë¡œìš´ ë„ì…ë¶€
            2. ì£¼ìš” ë‚´ìš© (ì—°êµ¬ ê²°ê³¼ ê¸°ë°˜)
            3. ì‹¤ìƒí™œ/ì‚°ì—… ì ìš© ì‚¬ë¡€
            4. ë¯¸ë˜ ì „ë§
            5. ê²°ë¡  ë° ìš”ì•½
            
            **ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°**: ë‹¨ í•œ ë‹¨ì–´ë„ ì˜ì–´ë¡œ ì‘ì„±í•˜ì§€ ë§ê³ , ëª¨ë“  ë‚´ìš©ì„ {self.config.language}ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
            
            ëŒ€ìƒ ë…ì: í•´ë‹¹ ë¶„ì•¼ì— ê´€ì‹¬ìˆëŠ” ì¼ë°˜ì¸ ë° ì „ë¬¸ê°€''',
            
            expected_output=f'''ë…ì ì¹œí™”ì ì´ê³  ì •ë³´ê°€ í’ë¶€í•œ {self.config.word_count_range[0]}-{self.config.word_count_range[1]}ë‹¨ì–´ ë¶„ëŸ‰ì˜ 
            {self.config.report_type}. **ì™„ì „íˆ {self.config.language}ë¡œë§Œ ì‘ì„±**ë˜ì—ˆìœ¼ë©°, 
            í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ë¡œ ì¶”ì¶œí•œ ì‹¤ì œ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ë°˜ì˜í•œ í˜„ì‹¤ì ì´ê³  ìœ ìš©í•œ ë‚´ìš© í¬í•¨.''',
            
            agent=writer,
            context=[research_task]
        )
        
        return planning_task, research_task, write_task
    
    def save_result(self, result):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ë™ì¼)"""
        if not result:
            logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"research_report_{self.config.safe_topic}_{timestamp}.md"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"# {self.config.topic} - ì—°êµ¬ ë³´ê³ ì„œ\n")
                f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ë³´ê³ ì„œ ìœ í˜•: {self.config.report_type}\n")
                f.write(f"ì–¸ì–´: {self.config.language}\n")
                f.write(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜: {self.config.search_queries_count}ê°œ\n")
                f.write(f"í˜ì´ì§€ë‹¹ ìµœëŒ€ í¬ë¡¤ë§: {self.config.max_pages_per_query}ê°œ\n\n")
                f.write("---\n\n")
                f.write(str(result))
            
            logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def research(self):
        """ë©”ì¸ ë¦¬ì„œì¹˜ ì‹¤í–‰ ë©”ì„œë“œ (ë™ì¼)"""
        try:
            logger.info("=" * 60)
            logger.info(f"ğŸš€ ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ ì‹œì‘")
            logger.info(f"ğŸ“‹ ì£¼ì œ: {self.config.topic}")
            logger.info(f"ğŸ“Š ë³´ê³ ì„œ ìœ í˜•: {self.config.report_type}")
            logger.info(f"ğŸ” í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© (ê²€ìƒ‰+í¬ë¡¤ë§+í…ìŠ¤íŠ¸ì¶”ì¶œ)")
            logger.info("=" * 60)
            
            # ì—ì´ì „íŠ¸ ë° ì‘ì—… ìƒì„±
            planner, researcher, writer = self.create_agents()
            planning_task, research_task, write_task = self.create_tasks(planner, researcher, writer)
            
            # í¬ë£¨ ì‹¤í–‰
            crew = Crew(
                agents=[planner, researcher, writer],
                tasks=[planning_task, research_task, write_task],
                process=Process.sequential,
                verbose=True,
                max_execution_time=MAX_EXECUTION_TIME
            )
            
            logger.info(f"\nğŸ¯ AI í¬ë£¨ ì‘ì—… ì‹œì‘: {self.config.topic}")
            logger.info("ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-8ë¶„ (í˜ì´ì§€ í¬ë¡¤ë§ í¬í•¨)")
            
            result = crew.kickoff()
            
            # ê²°ê³¼ ì €ì¥
            saved_file = self.save_result(result)
            
            logger.info("\nğŸ‰ í¬ë£¨ ì‘ì—… ì™„ë£Œ!")
            print(f"\nğŸ“„ ìƒì„±ëœ {self.config.report_type}:")
            print("=" * 80)
            print(result)
            print("=" * 80)
            
            if saved_file:
                logger.info(f"\nğŸ“ ê²°ê³¼ê°€ '{saved_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì„œì¹˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
            return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê¸°ì¡´ ë™ì¼)"""
    parser = argparse.ArgumentParser(description='ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ - í†µí•© ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ê¹Šì´ ìˆëŠ” ë³´ê³ ì„œ ìƒì„±')
    parser.add_argument('--topic', '-t', 
                        default='2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ', 
                        help='ì—°êµ¬ ì£¼ì œ (ë˜ëŠ” í”„ë¦¬ì…‹: ai, blockchain, health, etc.)')
    parser.add_argument('--queries', '-q', 
                        type=int, default=5, 
                        help='ê²€ìƒ‰ ì¿¼ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)')
    parser.add_argument('--pages', '-p', 
                        type=int, default=3, 
                        help='ì¿¼ë¦¬ë‹¹ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 3)')
    parser.add_argument('--words', '-w', 
                        default='700,900', 
                        help='ë‹¨ì–´ ìˆ˜ ë²”ìœ„ (ì˜ˆ: 700,900)')
    parser.add_argument('--type', '-r', 
                        default='ë¸”ë¡œê·¸', 
                        help='ë³´ê³ ì„œ ìœ í˜• (ë¸”ë¡œê·¸, ë³´ê³ ì„œ, ë¶„ì„ì„œ ë“±)')
    parser.add_argument('--language', '-l', 
                        default='í•œêµ­ì–´', 
                        help='ì¶œë ¥ ì–¸ì–´')
    parser.add_argument('--list-presets', 
                        action='store_true', 
                        help='ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ì£¼ì œ ëª©ë¡ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # í”„ë¦¬ì…‹ ëª©ë¡ ì¶œë ¥
    if args.list_presets:
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ì£¼ì œ:")
        for key, value in RESEARCH_PRESETS.items():
            print(f"  {key}: {value}")
        return
    
    # ì„¤ì • ìƒì„±
    try:
        word_range = tuple(map(int, args.words.split(',')))
        if len(word_range) != 2:
            raise ValueError("ë‹¨ì–´ ìˆ˜ëŠ” 'ìµœì†Œ,ìµœëŒ€' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”")
    except:
        word_range = (700, 900)
        print("âš ï¸ ë‹¨ì–´ ìˆ˜ í˜•ì‹ ì˜¤ë¥˜. ê¸°ë³¸ê°’ (700,900) ì‚¬ìš©")
    
    # í”„ë¦¬ì…‹ í™•ì¸ ë° ì£¼ì œ ì„¤ì •
    topic = get_preset_topic(args.topic)
    
    config = ResearchConfig(
        topic=topic,
        search_queries_count=args.queries,
        max_pages_per_query=args.pages,
        word_count_range=word_range,
        language=args.language,
        report_type=args.type
    )
    
    print(f"ğŸ¯ ì—°êµ¬ ì£¼ì œ: {config.topic}")
    print(f"ğŸ“Š ë³´ê³ ì„œ ìœ í˜•: {config.report_type}")
    print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {config.search_queries_count}ê°œ")
    print(f"ğŸ“„ ì¿¼ë¦¬ë‹¹ í¬ë¡¤ë§: ìµœëŒ€ {config.max_pages_per_query}ê°œ í˜ì´ì§€")
    print(f"ğŸ“ ëª©í‘œ ë‹¨ì–´ ìˆ˜: {config.word_count_range[0]}-{config.word_count_range[1]}ë‹¨ì–´")
    
    # ë¦¬ì„œì¹˜ ì‹¤í–‰
    crew = UniversalResearchCrew(config)
    result = crew.research()
    
    if result:
        print(f"\nâœ… '{config.topic}' ì—°êµ¬ ë³´ê³ ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâŒ ì‘ì—… ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")

def run_default():
    """ê¸°ë³¸ ì‹¤í–‰ í•¨ìˆ˜ (ë™ì¼)"""
    print("ğŸš€ ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ ì‹œì‘!")
    print("ğŸ“‹ ê¸°ë³¸ ì£¼ì œë¡œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì—°êµ¬ ì‹¤í–‰
    config = ResearchConfig(
        topic="2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ",
        search_queries_count=5,
        max_pages_per_query=3,
        word_count_range=(700, 900),
        language="í•œêµ­ì–´",
        report_type="ë¸”ë¡œê·¸"
    )
    
    print(f"ğŸ¯ ì—°êµ¬ ì£¼ì œ: {config.topic}")
    print(f"ğŸ“Š ë³´ê³ ì„œ ìœ í˜•: {config.report_type}")
    print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {config.search_queries_count}ê°œ")
    print(f"ğŸ“„ ì¿¼ë¦¬ë‹¹ í¬ë¡¤ë§: ìµœëŒ€ {config.max_pages_per_query}ê°œ í˜ì´ì§€")
    print(f"ğŸ“ ëª©í‘œ ë‹¨ì–´ ìˆ˜: {config.word_count_range[0]}-{config.word_count_range[1]}ë‹¨ì–´")
    print("\n" + "="*60)
    
    # ë¦¬ì„œì¹˜ ì‹¤í–‰
    crew = UniversalResearchCrew(config)
    result = crew.research()
    
    if result:
        print(f"\nâœ… '{config.topic}' ì—°êµ¬ ë³´ê³ ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ë‹¤ë¥¸ ì£¼ì œë¡œ ì—°êµ¬í•˜ë ¤ë©´: python script.py --topic 'ì›í•˜ëŠ” ì£¼ì œ'")
    else:
        print(f"\nâŒ ì‘ì—… ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
    
    return result

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìê°€ ìˆìœ¼ë©´ CLI ëª¨ë“œ, ì—†ìœ¼ë©´ ê¸°ë³¸ ì‹¤í–‰
    import sys
    if len(sys.argv) > 1:
        main()  # CLI ëª¨ë“œ
    else:
        run_default()  # ê¸°ë³¸ ì‹¤í–‰ ëª¨ë“œ