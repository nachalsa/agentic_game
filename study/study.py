import os
import logging
from datetime import datetime
import litellm
from crewai import Agent, Task, Crew, Process
from crewai.tools import tool
from dotenv import load_dotenv
import argparse
import re
import requests
from urllib.parse import urljoin, urlparse
import time
import random

from ddgs import DDGS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'research_crew_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ì„¤ì •
load_dotenv()
os.environ.setdefault("OTEL_SDK_DISABLED", "true")
os.environ.setdefault("CREWAI_DISABLE_TELEMETRY", "true")

# ì„¤ì • í´ë˜ìŠ¤
class ResearchConfig:
    """ë¦¬ì„œì¹˜ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 topic: str,
                 search_queries_count: int = 5,
                 max_pages_per_query: int = 3,
                 word_count_range: tuple = (700, 900),
                 language: str = "í•œêµ­ì–´",
                 report_type: str = "ë¸”ë¡œê·¸"):
        self.topic = topic
        self.search_queries_count = search_queries_count
        self.max_pages_per_query = max_pages_per_query
        self.word_count_range = word_count_range
        self.language = language
        self.report_type = report_type
        
        # íŒŒì¼ëª…ìš© ì•ˆì „í•œ í† í”½ëª… ìƒì„±
        self.safe_topic = re.sub(r'[^\w\s-]', '', topic.replace(' ', '_'))[:50]

# ìŠ¤ë§ˆíŠ¸ LLM ì„¤ì • í•¨ìˆ˜
def setup_llm_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ LLM ì„¤ì •ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì„¤ì •"""
    
    # ê¸°ë³¸ í™˜ê²½ë³€ìˆ˜ë“¤
    model_name = os.getenv("DEFAULT_LLM", "cpatonn/Devstral-Small-2507-AWQ")
    api_key = os.getenv("DEFAULT_API_KEY", "http://localhost:54321")
    api_base = os.getenv("DEFAULT_URL", None)
    
    # ëª¨ë¸ëª…ì—ì„œ í”„ë¡œë°”ì´ë” ìë™ ê°ì§€
    model_lower = model_name.lower()
    
    if any(x in model_lower for x in ["gemini", "google"]):
        # Gemini ì„¤ì •
        provider = "gemini"
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY", "")
        model_prefix = "gemini"
        api_base = None  # GeminiëŠ” ê¸°ë³¸ URL ì‚¬ìš©
        
    elif any(x in model_lower for x in ["gpt", "openai"]):
        # OpenAI ì„¤ì •
        provider = "openai"
        if not api_key:
            api_key = os.getenv("OPENAI_API_KEY", "")
        model_prefix = "openai"
        if not api_base:
            api_base = None  # OpenAI ê¸°ë³¸ URL ì‚¬ìš©
            
    elif any(x in model_lower for x in ["claude", "anthropic"]):
        # Anthropic ì„¤ì •
        provider = "anthropic"
        if not api_key:
            api_key = os.getenv("ANTHROPIC_API_KEY", "")
        model_prefix = "anthropic"
        api_base = None  # Anthropic ê¸°ë³¸ URL ì‚¬ìš©
        
    else:
        # Ollama/vLLM/ê¸°íƒ€ OpenAI í˜¸í™˜ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        provider = "openai_compatible"
        model_prefix = "openai"
        if not api_base:
            api_base = "http://localhost:11434"
        if not api_key:
            api_key = "ollama"
        
        # Ollama/vLLMì˜ OpenAI í˜¸í™˜ APIëŠ” /v1 ê²½ë¡œ í•„ìš”
        if api_base and not api_base.endswith('/v1'):
            api_base = api_base.rstrip('/') + '/v1'
    
    # ì „ì²´ ëª¨ë¸ëª… ìƒì„±
    if "/" in model_name:
        full_model_name = model_name  # ì´ë¯¸ í”„ë¦¬í”½ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
    else:
        full_model_name = f"{model_prefix}/{model_name}"
    
    # LiteLLM ì„¤ì •
    if api_base:
        litellm.api_base = api_base
        
    if api_key:
        litellm.api_key = api_key
        
    litellm.drop_params = True
    
    # ì •ë³´ ì¶œë ¥
    logger.info(f"ğŸ¤– í”„ë¡œë°”ì´ë”: {provider}")
    logger.info(f"ğŸ¤– ëª¨ë¸: {full_model_name}")
    logger.info(f"ğŸ”— API Base: {api_base or 'ê¸°ë³¸ê°’ ì‚¬ìš©'}")
    logger.info(f"ğŸ”‘ API í‚¤: {'ì„¤ì •ë¨' if api_key else 'ì—†ìŒ'}")
    
    return {
        "provider": provider,
        "full_model_name": full_model_name,
        "api_base": api_base,
        "api_key": api_key
    }

# í™˜ê²½ ë³€ìˆ˜ (ê¸°ì¡´ ìœ ì§€)
TIMEOUT = int(os.getenv("TIMEOUT", "30"))
MAX_EXECUTION_TIME = int(os.getenv("MAX_EXECUTION_TIME", "900"))

# ê°œì„ ëœ User-Agent ëª©ë¡
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
]

# Helper í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
def get_random_headers():
    """ëœë¤í•œ í—¤ë” ìƒì„±"""
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Referer': 'https://www.google.com/',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

def is_good_text(text):
    """í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦"""
    if not text or len(text.strip()) < 50:
        return False
    
    # JavaScript ì½”ë“œë‚˜ ì—ëŸ¬ ê°ì§€
    js_indicators = [
        'function(', '.push([', 'self.__next_f', 'window.', 
        'document.', 'var ', 'const ', 'let ', 'getElementById',
        'addEventListener', 'querySelector', '$(', 'jQuery'
    ]
    
    if any(indicator in text for indicator in js_indicators):
        return False
    
    # ì—ëŸ¬ ë©”ì‹œì§€ ê°ì§€
    error_indicators = [
        'Page not found', '404', '403', 'Access denied',
        'Forbidden', 'Error', 'exception', 'stacktrace'
    ]
    
    lower_text = text.lower()
    if any(error in lower_text for error in error_indicators):
        return False
    
    # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨ í™•ì¸
    words = text.split()
    if len(words) < 15:
        return False
        
    return True

def extract_with_requests_only(url):
    """requests + trafilaturaë§Œìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        logger.info(f"ğŸ“„ requests + trafilaturaë¡œ ì¶”ì¶œ ì‹œë„: {url}")
        
        headers = get_random_headers()
        
        # ì—¬ëŸ¬ ë²ˆ ì‹œë„
        for attempt in range(2):
            try:
                response = requests.get(
                    url, 
                    headers=headers, 
                    timeout=15,
                    allow_redirects=True,
                    verify=False
                )
                
                if response.status_code == 200:
                    break
                elif response.status_code == 403:
                    logger.warning(f"âš ï¸ 403 ì—ëŸ¬, ë‹¤ë¥¸ í—¤ë”ë¡œ ì¬ì‹œë„: {url}")
                    headers = get_random_headers()
                    time.sleep(1)
                    continue
                else:
                    logger.warning(f"âš ï¸ HTTP {response.status_code}: {url}")
                    return None
                    
            except requests.RequestException as e:
                logger.warning(f"âš ï¸ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {str(e)}")
                if attempt == 0:
                    time.sleep(2)
                    continue
                return None
        
        if response.status_code != 200:
            return None
            
        # trafilaturaë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        try:
            import trafilatura
            
            extracted_text = trafilatura.extract(
                response.text,
                include_comments=False,
                include_tables=True,
                include_images=False,
            )
            
            if extracted_text and is_good_text(extracted_text):
                clean_text = extracted_text.strip()
                clean_text = re.sub(r'\n{3,}', '\n\n', clean_text)
                if len(clean_text) > 3000:
                    clean_text = clean_text[:3000] + "..."
                
                logger.info(f"âœ… requests+trafilatura ì„±ê³µ: {len(clean_text)}ì")
                return clean_text
                
        except ImportError:
            logger.error("âŒ trafilaturaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ trafilatura ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
        return None
        
    except Exception as e:
        logger.warning(f"âš ï¸ requests ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

def extract_with_playwright_improved(url):
    """ê°œì„ ëœ Playwright ë°±ì—… ì¶”ì¶œ"""
    try:
        from playwright.sync_api import sync_playwright
        
        logger.info(f"ğŸ­ Playwright ë°±ì—… ì‹œë„: {url}")
        
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-images',
                    '--disable-javascript',
                    '--disable-plugins',
                    '--disable-extensions'
                ]
            )
            
            context = browser.new_context(
                viewport={'width': 1280, 'height': 720},
                user_agent=random.choice(USER_AGENTS)
            )
            
            page = context.new_page()
            
            try:
                page.goto(url, wait_until='domcontentloaded', timeout=15000)
                time.sleep(1)
                content = page.content()
            except Exception as e:
                logger.warning(f"âš ï¸ Playwright í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                return None
            finally:
                browser.close()
            
            # trafilaturaë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                import trafilatura
                extracted_text = trafilatura.extract(
                    content,
                    include_comments=False,
                    include_tables=True,
                    include_images=False
                )
                
                if extracted_text and is_good_text(extracted_text):
                    clean_text = extracted_text.strip()
                    clean_text = re.sub(r'\n{3,}', '\n\n', clean_text)
                    if len(clean_text) > 3000:
                        clean_text = clean_text[:3000] + "..."
                    
                    logger.info(f"âœ… Playwright ì„±ê³µ: {len(clean_text)}ì")
                    return clean_text
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Playwright trafilatura ì‹¤íŒ¨: {str(e)}")
                
        return None
        
    except ImportError:
        logger.warning("âš ï¸ Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return None
    except Exception as e:
        logger.warning(f"âš ï¸ Playwright ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

def fallback_simple_extraction(url):
    """ìµœí›„ì˜ ìˆ˜ë‹¨: ê°„ë‹¨í•œ HTML íŒŒì‹±"""
    try:
        logger.info(f"ğŸ”§ ê°„ë‹¨í•œ HTML íŒŒì‹± ì‹œë„: {url}")
        
        headers = get_random_headers()
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        
        if response.status_code != 200:
            return None
            
        # ê°„ë‹¨í•œ HTML íƒœê·¸ ì œê±°
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ ì œê±°
        for script in soup(["script", "style"]):
            script.decompose()
            
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        if is_good_text(text):
            if len(text) > 2000:
                text = text[:2000] + "..."
            logger.info(f"âœ… ê°„ë‹¨í•œ íŒŒì‹± ì„±ê³µ: {len(text)}ì")
            return text
            
    except ImportError:
        logger.warning("âš ï¸ BeautifulSoupì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.warning(f"âš ï¸ ê°„ë‹¨í•œ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        
    return None

# ì›¹ ê²€ìƒ‰ ë„êµ¬ (ê¸°ì¡´ê³¼ ë™ì¼)
@tool("Web Search Tool")
def web_search_tool(query: str) -> str:
    """ê°œì„ ëœ í†µí•© ì›¹ ê²€ìƒ‰ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ë„êµ¬"""
    try:
        logger.info(f"ğŸ” ê°œì„ ëœ ì›¹ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # 1ë‹¨ê³„: ì›¹ ê²€ìƒ‰
        ddgs = DDGS()
        
        try:
            search_results = ddgs.text(
                query=query, 
                region='wt-wt', 
                safesearch='moderate', 
                max_results=8
            )
        except Exception as e:
            logger.warning(f"âš ï¸ DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return f"'{query}' ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        if not search_results:
            logger.warning(f"âš ï¸ '{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¤‘ë³µ URL ì œê±° ë° í•„í„°ë§
        unique_urls = []
        seen_urls = set()
        
        blocked_domains = [
            'microsoft.com', 'apple.com', 'facebook.com', 'twitter.com',
            'linkedin.com', 'indeed.com', 'glassdoor.com'
        ]
        
        for result in search_results:
            url = result.get('href', '')
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            
            if url and url not in seen_urls:
                domain_blocked = any(domain in url.lower() for domain in blocked_domains)
                if not domain_blocked:
                    unique_urls.append({'url': url, 'title': title})
                    seen_urls.add(url)
                else:
                    logger.info(f"âš ï¸ ì°¨ë‹¨ëœ ë„ë©”ì¸ ê±´ë„ˆëœ€: {url}")
        
        if not unique_urls:
            return f"'{query}'ì— ëŒ€í•œ ì ‘ê·¼ ê°€ëŠ¥í•œ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 2ë‹¨ê³„: í˜ì´ì§€ í¬ë¡¤ë§ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_contents = []
        max_pages = min(4, len(unique_urls))
        
        for i, item in enumerate(unique_urls[:max_pages]):
            url = item['url']
            title = item['title']
            
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ({i+1}/{max_pages}): {url}")
            
            # ë‹¤ë‹¨ê³„ ì¶”ì¶œ ì‹œë„
            extracted_text = None
            
            # 1ì°¨: requests + trafilatura
            extracted_text = extract_with_requests_only(url)
            
            # 2ì°¨: Playwright ë°±ì—…
            if not extracted_text:
                extracted_text = extract_with_playwright_improved(url)
            
            # 3ì°¨: ê°„ë‹¨í•œ HTML íŒŒì‹±
            if not extracted_text:
                extracted_text = fallback_simple_extraction(url)
            
            if extracted_text:
                extracted_contents.append({
                    'title': title,
                    'url': url,
                    'content': extracted_text,
                    'method': 'multi-stage'
                })
                logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(extracted_text)}ì")
            else:
                logger.warning(f"âš ï¸ ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨: {url}")
            
            time.sleep(random.uniform(1, 2))
        
        # 3ë‹¨ê³„: ê²°ê³¼ í¬ë§·íŒ…
        if not extracted_contents:
            return f"'{query}' ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”."
        
        formatted_result = f"ğŸ” '{query}' ê²€ìƒ‰ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ê²°ê³¼:\n\n"
        
        for i, content in enumerate(extracted_contents, 1):
            formatted_result += f"ğŸ“„ {i}. {content['title']}\n"
            formatted_result += f"ğŸ”— ì¶œì²˜: {content['url']}\n"
            formatted_result += f"ğŸ“ ë‚´ìš©:\n{content['content']}\n"
            formatted_result += "-" * 80 + "\n\n"
        
        logger.info(f"âœ… í†µí•© ê²€ìƒ‰ ì™„ë£Œ: {len(extracted_contents)}ê°œ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        return formatted_result
        
    except Exception as e:
        error_msg = f"âŒ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        return error_msg

# ì£¼ì œë³„ í”„ë¦¬ì…‹ (ê¸°ì¡´ê³¼ ë™ì¼)
RESEARCH_PRESETS = {
    "ai": "2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ",
    "blockchain": "2025ë…„ ë¸”ë¡ì²´ì¸ ê¸°ìˆ  ë°œì „", 
    "climate": "ì§€ì†ê°€ëŠ¥í•œ ê¸°í›„ ê¸°ìˆ  í˜ì‹ ",
    "health": "ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ê¸°ìˆ  íŠ¸ë Œë“œ",
    "fintech": "í•€í…Œí¬ ì‚°ì—… ìµœì‹  ë™í–¥",
    "architecture": "í˜„ëŒ€ ê±´ì¶• ê¸°ìˆ  í˜ì‹ ",
    "education": "êµìœ¡ ê¸°ìˆ  ë””ì§€í„¸ ì „í™˜",
    "energy": "ì¬ìƒ ì—ë„ˆì§€ ê¸°ìˆ  ë°œì „",
    "space": "ìš°ì£¼ ê¸°ìˆ  ë° íƒì‚¬ ë™í–¥",
    "food": "í‘¸ë“œí…Œí¬ ì‚°ì—… í˜ì‹ "
}

def get_preset_topic(preset_name: str) -> str:
    """í”„ë¦¬ì…‹ ì£¼ì œ ë°˜í™˜ (ì—†ìœ¼ë©´ ì…ë ¥ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜)"""
    return RESEARCH_PRESETS.get(preset_name.lower(), preset_name)

# ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ í´ë˜ìŠ¤ (ê°„ì†Œí™”)
class UniversalResearchCrew:
    """ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ë¦¬ì„œì¹˜ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” AI í¬ë£¨ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: ResearchConfig):
        self.config = config
        self.llm_config = setup_llm_config()
    
    def create_agents(self):
        """ì—ì´ì „íŠ¸ ìƒì„±"""
        
        # ì„¤ì •ëœ ëª¨ë¸ëª… ì‚¬ìš©
        full_model_name = self.llm_config["full_model_name"]
        
        planner = Agent(
            role='ì—°êµ¬ ê³„íš ì „ë¬¸ê°€',
            goal=f'{self.config.topic}ì— ëŒ€í•œ íš¨ê³¼ì ì¸ ì›¹ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½',
            backstory='''ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì „ëµê°€ì…ë‹ˆë‹¤. 
            ë³µì¡í•œ ì£¼ì œë¥¼ í•µì‹¬ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ê³ , ìµœì‹  ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ê²€ìƒ‰ì–´ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.''',
            verbose=True,
            allow_delegation=False,
            llm=full_model_name,
            max_tokens=1024,
            temperature=0.6
        )

        researcher = Agent(
            role='ì „ë¬¸ ë¦¬ì„œì¹˜ ë¶„ì„ê°€',
            goal=f'{self.config.topic}ì— ëŒ€í•œ ì¢…í•©ì ì´ê³  ì‹¬ì¸µì ì¸ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„',
            backstory='''í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ , 
            ë‹¤ë‹¨ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë°©ë²•ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ìˆ™ë ¨ëœ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.''',
            verbose=True,
            allow_delegation=False,
            tools=[web_search_tool],
            llm=full_model_name,
            max_tokens=2000,
            temperature=0.7
        )

        writer = Agent(
            role='ì „ë¬¸ ì½˜í…ì¸  ì‘ê°€',
            goal=f'{self.config.topic}ì— ëŒ€í•œ ë§¤ë ¥ì ì´ê³  ìœ ìµí•œ {self.config.report_type} ì‘ì„±',
            backstory=f'''ë³µì¡í•œ ì •ë³´ë¥¼ {self.config.language}ë¡œ ëª…í™•í•˜ê³  ë§¤ë ¥ì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ì „ë¬¸ ì‘ê°€ì…ë‹ˆë‹¤. 
            ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ìµœì‹  ì •ë³´ë¥¼ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ ì½˜í…ì¸ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.''',
            verbose=True,
            allow_delegation=False,
            llm=full_model_name,
            max_tokens=2000,
            temperature=0.8
        )
        
        return planner, researcher, writer
    
    def create_tasks(self, planner, researcher, writer):
        """íƒœìŠ¤í¬ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)"""
        
        # 1. ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½
        planning_task = Task(
            description=f'''"{self.config.topic}"ì— ëŒ€í•œ í¬ê´„ì ì¸ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
            
            ì´ ì£¼ì œë¥¼ ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•˜ì—¬ {self.config.search_queries_count}ê°œì˜ êµ¬ì²´ì ì´ê³  íš¨ê³¼ì ì¸ ì˜ì–´ ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
            
            1. ìµœì‹  ë™í–¥ ë° ë°œì „ì‚¬í•­ (latest trends, developments, innovations)
            2. ì „ë¬¸ê°€ ë¶„ì„ ë° ì—°êµ¬ ê²°ê³¼ (expert analysis, research, studies)  
            3. ì‹¤ì œ ì ìš© ì‚¬ë¡€ ë° ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” (case studies, applications, examples)
            4. ë¯¸ë˜ ì „ë§ ë° ì˜ˆì¸¡ (future outlook, predictions, forecasts)
            5. ì‚°ì—…ë³„ ì˜í–¥ ë° í™œìš© (industry impact, implementation)
            
            **ì¤‘ìš”í•œ í˜•ì‹ ìš”êµ¬ì‚¬í•­:**
            - ê° ì¿¼ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
            - ì •í™•íˆ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
            SEARCH_QUERY_1: "ì²« ë²ˆì§¸ ê²€ìƒ‰ì–´"
            SEARCH_QUERY_2: "ë‘ ë²ˆì§¸ ê²€ìƒ‰ì–´"  
            SEARCH_QUERY_3: "ì„¸ ë²ˆì§¸ ê²€ìƒ‰ì–´"
            SEARCH_QUERY_4: "ë„¤ ë²ˆì§¸ ê²€ìƒ‰ì–´"
            SEARCH_QUERY_5: "ë‹¤ì„¯ ë²ˆì§¸ ê²€ìƒ‰ì–´"
            
            ê° ì¿¼ë¦¬ëŠ” ëª…í™•í•˜ê³  ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥í•´ì•¼ í•˜ë©°, 2024-2025ë…„ ìµœì‹  ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•˜ì„¸ìš”.''',
            
            expected_output=f'''ì •í™•íˆ ë‹¤ìŒ í˜•ì‹ì˜ {self.config.search_queries_count}ê°œ ì˜ì–´ ê²€ìƒ‰ ì¿¼ë¦¬:
            SEARCH_QUERY_1: "query1"
            SEARCH_QUERY_2: "query2"
            SEARCH_QUERY_3: "query3"
            SEARCH_QUERY_4: "query4"  
            SEARCH_QUERY_5: "query5"
            ì£¼ì œ: {self.config.topic}ì— ìµœì í™”ëœ ì„œë¡œ ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë“¤''',
            agent=planner
        )
        
        # 2. ì •ë³´ ìˆ˜ì§‘
        research_task = Task(
            description=f'''ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ëª©ë¡ì„ í™œìš©í•˜ì—¬ "{self.config.topic}"ì— ëŒ€í•œ ì‹¬ì¸µ ì›¹ ê²€ìƒ‰ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

            **í•„ìˆ˜ ìˆ˜í–‰ ì ˆì°¨:**
            1. ì´ì „ Task ê²°ê³¼ì—ì„œ "SEARCH_QUERY_1:", "SEARCH_QUERY_2:" ë“±ì˜ í˜•ì‹ìœ¼ë¡œ ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ì„ ì°¾ì•„ ì¶”ì¶œí•©ë‹ˆë‹¤.
            2. ê° SEARCH_QUERY_Xì—ì„œ ë”°ì˜´í‘œ ì•ˆì˜ ê²€ìƒ‰ì–´ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            3. ì¶”ì¶œëœ **ëª¨ë“  ê²€ìƒ‰ì–´ë¥¼ í•˜ë‚˜ì”© ìˆœì„œëŒ€ë¡œ** 'Web Search Tool'ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            4. í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ê°€ ìë™ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
               - ì›¹ ê²€ìƒ‰ ì‹¤í–‰
               - ë‹¤ë‹¨ê³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (requests + trafilatura â†’ Playwright â†’ ê°„ë‹¨ íŒŒì‹±)
               - ì°¨ë‹¨ëœ ë„ë©”ì¸ ìš°íšŒ
               - ì •ì œëœ í…ìŠ¤íŠ¸ ê²°ê³¼ ë°˜í™˜
            5. ê²€ìƒ‰í•  ë•Œë§ˆë‹¤ "ğŸ” ê²€ìƒ‰ ì¤‘: X/{self.config.search_queries_count} - [ê²€ìƒ‰ì–´]" í˜•íƒœë¡œ ì§„í–‰ìƒí™©ì„ ì•Œë ¤ì£¼ì„¸ìš”.
            
            **ë³´ê³ ì„œ ì‘ì„± ìš”êµ¬ì‚¬í•­:**
            ëª¨ë“  ê²€ìƒ‰ ì™„ë£Œ í›„, ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í¬í•¨í•œ ì¢…í•© ë³´ê³ ì„œë¥¼ **ë°˜ë“œì‹œ {self.config.language}ë¡œ** ì‘ì„±í•˜ì„¸ìš”:
            - ì£¼ìš” íŠ¸ë Œë“œ ë° ë°œì „ì‚¬í•­
            - í•µì‹¬ í†µê³„ ë° ë°ì´í„°  
            - êµ¬ì²´ì  ì‚¬ë¡€ ë° ì‹¤ë¬´ ì ìš©
            - ì „ë¬¸ê°€ ì˜ê²¬ ë° ë¶„ì„
            - ë¯¸ë˜ ì „ë§
            
            **ì ˆëŒ€ì ìœ¼ë¡œ ì¤‘ìš”**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì˜ì–´ë¡œ ë‚˜ì™€ë„ ë³´ê³ ì„œëŠ” **ë¬´ì¡°ê±´ {self.config.language}ë¡œë§Œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.''',
            
            expected_output=f'''"{self.config.topic}"ì— ëŒ€í•œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸, ìµœì‹  í†µê³„ ë° ì‹¤ì œ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ëŠ” 
            400-500ë‹¨ì–´ ë¶„ëŸ‰ì˜ ìƒì„¸í•œ ì—°êµ¬ ìš”ì•½ ë³´ê³ ì„œ (**ë°˜ë“œì‹œ {self.config.language}ë¡œ ì‘ì„±**).
            í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ë¡œ ì¶”ì¶œí•œ ì‹¤ì œ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±.''',
            
            agent=researcher,
            context=[planning_task]
        )

        # 3. ì½˜í…ì¸  ì‘ì„±
        write_task = Task(
            description=f'''ì—°êµ¬ ìš”ì•½ ë³´ê³ ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ "{self.config.topic}"ì— ëŒ€í•œ 
            **ë°˜ë“œì‹œ {self.config.language}ë¡œë§Œ ì‘ì„±ëœ** {self.config.report_type}ì„ ì‘ì„±í•©ë‹ˆë‹¤.
            
            **ì ˆëŒ€ì  ì–¸ì–´ ìš”êµ¬ì‚¬í•­:**
            - **ëª¨ë“  ë‚´ìš©ì€ {self.config.language}ë¡œë§Œ ì‘ì„±**í•´ì•¼ í•©ë‹ˆë‹¤
            - ì œëª©, ì†Œì œëª©, ë³¸ë¬¸, ëª¨ë“  í…ìŠ¤íŠ¸ê°€ {self.config.language}ì—¬ì•¼ í•©ë‹ˆë‹¤
            - ì˜ì–´ ë‹¨ì–´ë‚˜ ë¬¸ì¥ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
            
            **êµ¬ì²´ì  ìš”êµ¬ì‚¬í•­:**
            - ë¶„ëŸ‰: {self.config.word_count_range[0]}-{self.config.word_count_range[1]}ë‹¨ì–´
            - ë§¤ë ¥ì ì¸ {self.config.language} ì œëª©ê³¼ ë¶€ì œëª© ì‚¬ìš©
            - ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ {self.config.language} í‘œí˜„
            - ì—°êµ¬ ë³´ê³ ì„œì˜ ìµœì‹  ì •ë³´ë¥¼ ë°˜ì˜í•œ í˜„ì‹¤ì ì¸ ë‚´ìš©
            - ë…ìì˜ ê´€ì‹¬ì„ ë„ëŠ” êµ¬ì„±
            - ì‹¤ì œ ì‚¬ë¡€ë‚˜ êµ¬ì²´ì  ì˜ˆì‹œ í¬í•¨
            
            **êµ¬ì¡° ({self.config.language}ë¡œ ì‘ì„±):**
            1. í¥ë¯¸ë¡œìš´ ë„ì…ë¶€
            2. ì£¼ìš” ë‚´ìš© (ì—°êµ¬ ê²°ê³¼ ê¸°ë°˜)
            3. ì‹¤ìƒí™œ/ì‚°ì—… ì ìš© ì‚¬ë¡€
            4. ë¯¸ë˜ ì „ë§
            5. ê²°ë¡  ë° ìš”ì•½
            
            **ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°**: ë‹¨ í•œ ë‹¨ì–´ë„ ì˜ì–´ë¡œ ì‘ì„±í•˜ì§€ ë§ê³ , ëª¨ë“  ë‚´ìš©ì„ {self.config.language}ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.
            
            ëŒ€ìƒ ë…ì: í•´ë‹¹ ë¶„ì•¼ì— ê´€ì‹¬ìˆëŠ” ì¼ë°˜ì¸ ë° ì „ë¬¸ê°€''',
            
            expected_output=f'''ë…ì ì¹œí™”ì ì´ê³  ì •ë³´ê°€ í’ë¶€í•œ {self.config.word_count_range[0]}-{self.config.word_count_range[1]}ë‹¨ì–´ ë¶„ëŸ‰ì˜ 
            {self.config.report_type}. **ì™„ì „íˆ {self.config.language}ë¡œë§Œ ì‘ì„±**ë˜ì—ˆìœ¼ë©°, 
            í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ë¡œ ì¶”ì¶œí•œ ì‹¤ì œ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ë°˜ì˜í•œ í˜„ì‹¤ì ì´ê³  ìœ ìš©í•œ ë‚´ìš© í¬í•¨.''',
            
            agent=writer,
            context=[research_task]
        )
        
        return planning_task, research_task, write_task
    
    def save_result(self, result):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not result:
            logger.warning("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"research_report_{self.config.safe_topic}_{timestamp}.md"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"# {self.config.topic} - ì—°êµ¬ ë³´ê³ ì„œ\n")
                f.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ë³´ê³ ì„œ ìœ í˜•: {self.config.report_type}\n")
                f.write(f"ì–¸ì–´: {self.config.language}\n")
                f.write(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìˆ˜: {self.config.search_queries_count}ê°œ\n")
                f.write(f"í˜ì´ì§€ë‹¹ ìµœëŒ€ í¬ë¡¤ë§: {self.config.max_pages_per_query}ê°œ\n")
                f.write(f"LLM í”„ë¡œë°”ì´ë”: {self.llm_config['provider']}\n")
                f.write(f"LLM ëª¨ë¸: {self.llm_config['full_model_name']}\n")
                f.write("\n---\n\n")
                f.write(str(result))
            
            logger.info(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def research(self):
        """ë©”ì¸ ë¦¬ì„œì¹˜ ì‹¤í–‰ ë©”ì„œë“œ"""
        try:
            logger.info("=" * 60)
            logger.info(f"ğŸš€ ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ ì‹œì‘")
            logger.info(f"ğŸ“‹ ì£¼ì œ: {self.config.topic}")
            logger.info(f"ğŸ“Š ë³´ê³ ì„œ ìœ í˜•: {self.config.report_type}")
            logger.info(f"ğŸ” í†µí•© ì›¹ ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© (ê²€ìƒ‰+í¬ë¡¤ë§+í…ìŠ¤íŠ¸ì¶”ì¶œ)")
            logger.info("=" * 60)
            
            # ì—ì´ì „íŠ¸ ë° ì‘ì—… ìƒì„±
            planner, researcher, writer = self.create_agents()
            planning_task, research_task, write_task = self.create_tasks(planner, researcher, writer)
            
            # í¬ë£¨ ì‹¤í–‰
            crew = Crew(
                agents=[planner, researcher, writer],
                tasks=[planning_task, research_task, write_task],
                process=Process.sequential,
                verbose=True,
                max_execution_time=MAX_EXECUTION_TIME
            )
            
            logger.info(f"\nğŸ¯ AI í¬ë£¨ ì‘ì—… ì‹œì‘: {self.config.topic}")
            logger.info("ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-8ë¶„ (í˜ì´ì§€ í¬ë¡¤ë§ í¬í•¨)")
            
            result = crew.kickoff()
            
            # ê²°ê³¼ ì €ì¥
            saved_file = self.save_result(result)
            
            logger.info("\nğŸ‰ í¬ë£¨ ì‘ì—… ì™„ë£Œ!")
            print(f"\nğŸ“„ ìƒì„±ëœ {self.config.report_type}:")
            print("=" * 80)
            print(result)
            print("=" * 80)
            
            if saved_file:
                logger.info(f"\nğŸ“ ê²°ê³¼ê°€ '{saved_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì„œì¹˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
            return None

def test_llm_connection():
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ§ª LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # ì„¤ì • ì •ë³´ ì¶œë ¥
        llm_config = setup_llm_config()
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        test_messages = [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."}]
        
        start_time = time.time()
        response = litellm.completion(
            model=llm_config["full_model_name"],
            messages=test_messages,
            max_tokens=100,
            temperature=0.7
        )
        end_time = time.time()
        
        # ê²°ê³¼ ì¶œë ¥
        response_time = end_time - start_time
        content = response.choices[0].message.content
        
        print("âœ… ì—°ê²° ì„±ê³µ!")
        print(f"ğŸ“ ì‘ë‹µ: {content}")
        print(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
        
        if hasattr(response, 'usage') and response.usage:
            print(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: {response.usage.total_tokens}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        
        # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ í•´ê²°ì±… ì œì•ˆ
        error_str = str(e).lower()
        if "api key" in error_str or "authentication" in error_str:
            print("ğŸ’¡ í•´ê²°ì±…: API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
        elif "connection" in error_str or "timeout" in error_str:
            print("ğŸ’¡ í•´ê²°ì±…: ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
        elif "quota" in error_str or "rate limit" in error_str:
            print("ğŸ’¡ í•´ê²°ì±…: API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
        
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ - ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë” ì§€ì›')
    parser.add_argument('--topic', '-t', 
                        default='2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ', 
                        help='ì—°êµ¬ ì£¼ì œ (ë˜ëŠ” í”„ë¦¬ì…‹: ai, blockchain, health, etc.)')
    parser.add_argument('--queries', '-q', 
                        type=int, default=5, 
                        help='ê²€ìƒ‰ ì¿¼ë¦¬ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)')
    parser.add_argument('--pages', '-p', 
                        type=int, default=3, 
                        help='ì¿¼ë¦¬ë‹¹ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 3)')
    parser.add_argument('--words', '-w', 
                        default='700,900', 
                        help='ë‹¨ì–´ ìˆ˜ ë²”ìœ„ (ì˜ˆ: 700,900)')
    parser.add_argument('--type', '-r', 
                        default='ë¸”ë¡œê·¸', 
                        help='ë³´ê³ ì„œ ìœ í˜• (ë¸”ë¡œê·¸, ë³´ê³ ì„œ, ë¶„ì„ì„œ ë“±)')
    parser.add_argument('--language', '-l', 
                        default='í•œêµ­ì–´', 
                        help='ì¶œë ¥ ì–¸ì–´')
    parser.add_argument('--list-presets', 
                        action='store_true', 
                        help='ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ì£¼ì œ ëª©ë¡ ì¶œë ¥')
    parser.add_argument('--test-llm', 
                        action='store_true', 
                        help='LLM ì—°ê²° í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    # í”„ë¦¬ì…‹ ëª©ë¡ ì¶œë ¥
    if args.list_presets:
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ì£¼ì œ:")
        for key, value in RESEARCH_PRESETS.items():
            print(f"  {key}: {value}")
        return
    
    # LLM í…ŒìŠ¤íŠ¸
    if args.test_llm:
        test_llm_connection()
        return
    
    # ì„¤ì • ìƒì„±
    try:
        word_range = tuple(map(int, args.words.split(',')))
        if len(word_range) != 2:
            raise ValueError("ë‹¨ì–´ ìˆ˜ëŠ” 'ìµœì†Œ,ìµœëŒ€' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”")
    except:
        word_range = (700, 900)
        print("âš ï¸ ë‹¨ì–´ ìˆ˜ í˜•ì‹ ì˜¤ë¥˜. ê¸°ë³¸ê°’ (700,900) ì‚¬ìš©")
    
    # í”„ë¦¬ì…‹ í™•ì¸ ë° ì£¼ì œ ì„¤ì •
    topic = get_preset_topic(args.topic)
    
    config = ResearchConfig(
        topic=topic,
        search_queries_count=args.queries,
        max_pages_per_query=args.pages,
        word_count_range=word_range,
        language=args.language,
        report_type=args.type
    )
    
    print(f"ğŸ¯ ì—°êµ¬ ì£¼ì œ: {config.topic}")
    print(f"ğŸ“Š ë³´ê³ ì„œ ìœ í˜•: {config.report_type}")
    print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {config.search_queries_count}ê°œ")
    print(f"ğŸ“„ ì¿¼ë¦¬ë‹¹ í¬ë¡¤ë§: ìµœëŒ€ {config.max_pages_per_query}ê°œ í˜ì´ì§€")
    print(f"ğŸ“ ëª©í‘œ ë‹¨ì–´ ìˆ˜: {config.word_count_range[0]}-{config.word_count_range[1]}ë‹¨ì–´")
    
    # ë¦¬ì„œì¹˜ ì‹¤í–‰
    crew = UniversalResearchCrew(config)
    result = crew.research()
    
    if result:
        print(f"\nâœ… '{config.topic}' ì—°êµ¬ ë³´ê³ ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâŒ ì‘ì—… ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")

def run_default():
    """ê¸°ë³¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë²”ìš© AI ë¦¬ì„œì¹˜ í¬ë£¨ ì‹œì‘!")
    print("ğŸ“‹ ê¸°ë³¸ ì£¼ì œë¡œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì—°êµ¬ ì‹¤í–‰
    config = ResearchConfig(
        topic="2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œ",
        search_queries_count=5,
        max_pages_per_query=3,
        word_count_range=(700, 900),
        language="í•œêµ­ì–´",
        report_type="ë¸”ë¡œê·¸"
    )
    
    print(f"ğŸ¯ ì—°êµ¬ ì£¼ì œ: {config.topic}")
    print(f"ğŸ“Š ë³´ê³ ì„œ ìœ í˜•: {config.report_type}")
    print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {config.search_queries_count}ê°œ")
    print(f"ğŸ“„ ì¿¼ë¦¬ë‹¹ í¬ë¡¤ë§: ìµœëŒ€ {config.max_pages_per_query}ê°œ í˜ì´ì§€")
    print(f"ğŸ“ ëª©í‘œ ë‹¨ì–´ ìˆ˜: {config.word_count_range[0]}-{config.word_count_range[1]}ë‹¨ì–´")
    print("\n" + "="*60)
    
    # ë¦¬ì„œì¹˜ ì‹¤í–‰
    crew = UniversalResearchCrew(config)
    result = crew.research()
    
    if result:
        print(f"\nâœ… '{config.topic}' ì—°êµ¬ ë³´ê³ ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ë‹¤ë¥¸ ì£¼ì œë¡œ ì—°êµ¬í•˜ë ¤ë©´: python script.py --topic 'ì›í•˜ëŠ” ì£¼ì œ'")
        print("ğŸ’¡ LLM ì—°ê²° í…ŒìŠ¤íŠ¸: python script.py --test-llm")
    else:
        print(f"\nâŒ ì‘ì—… ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
    
    return result

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìê°€ ìˆìœ¼ë©´ CLI ëª¨ë“œ, ì—†ìœ¼ë©´ ê¸°ë³¸ ì‹¤í–‰
    import sys
    if len(sys.argv) > 1:
        main()  # CLI ëª¨ë“œ
    else:
        run_default()  # ê¸°ë³¸ ì‹¤í–‰ ëª¨ë“œ