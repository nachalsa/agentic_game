import os
import litellm
from crewai import Agent, Task, Crew, Process
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í¬ë£¨ AI í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
os.environ.setdefault("OTEL_SDK_DISABLED", "true")
# í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸° (ë˜ëŠ” ì§ì ‘ ì„¤ì •)
MODEL_NAME = os.getenv("DEFAULT_LLM", "cpatonn/Devstral-Small-2507-AWQ")
API_BASE_URL = os.getenv("DEFAULT_URL", "http://localhost:54321")
API_KEY = os.getenv("DEFAULT_API_KEY", "huntr/x_How_It's_Done")

# URL ì •ê·œí™” - /v1ì´ ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
if not API_BASE_URL.endswith('/v1'):
    API_BASE_URL = API_BASE_URL.rstrip('/') + '/v1'

print(f"MODEL_NAME: {MODEL_NAME}")
print(f"API_BASE_URL: {API_BASE_URL}")
print(f"API_KEY: {'****' if API_KEY else 'None'}")

# LiteLLM ê¸€ë¡œë²Œ ì„¤ì •
litellm.api_base = API_BASE_URL
litellm.api_key = API_KEY
litellm.drop_params = True  # ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ìë™ ì œê±°
litellm.set_verbose = True  # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸

def test_vllm_connection():
    """VLLM ì„œë²„ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        import requests
        # VLLM ì„œë²„ì˜ chat/completions ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
        chat_url = API_BASE_URL + '/chat/completions'
        test_data = {
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": "ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."}],
            "temperature": 0.7,
            "max_tokens": 50
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}"
        }
        
        response = requests.post(chat_url, json=test_data, headers=headers, timeout=30)
        print(f"VLLM ì„œë²„ ìƒíƒœ: {response.status_code}")
        
        if response.status_code == 200:
            print("VLLM ì„œë²„ ì—°ê²° ì„±ê³µ!")
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                test_response = result['choices'][0]['message']['content']
                print(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response}")
                return True
        else:
            print(f"VLLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.text}")
            return False
    except Exception as e:
        print(f"VLLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def test_litellm_connection():
    """LiteLLMì„ í†µí•œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        print("LiteLLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        response = litellm.completion(
            model=f"openai/{MODEL_NAME}",  # OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì§€ì •
            messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”, LiteLLM í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."}],
            api_base=API_BASE_URL,
            api_key=API_KEY,
            temperature=0.7,
            max_tokens=100,
            timeout=30
        )
        
        print(f"LiteLLM í…ŒìŠ¤íŠ¸ ì„±ê³µ: {response.choices[0].message.content}")
        return True
        
    except Exception as e:
        print(f"LiteLLM ì—°ê²° ì‹¤íŒ¨: {e}")
        print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        return False

# ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
print("=" * 50)
print("VLLM ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
if not test_vllm_connection():
    print("âŒ VLLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

print("\nLiteLLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
if not test_litellm_connection():
    print("âŒ LiteLLM ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    exit(1)

print("=" * 50)

# 1. ì—ì´ì „íŠ¸ ì •ì˜ (LiteLLM ì§ì ‘ ì‚¬ìš©)
researcher = Agent(
    role='ê³ ê¸‰ ì—°êµ¬ ë¶„ì„ê°€',
    goal='ìµœì‹  AI íŠ¸ë Œë“œì— ëŒ€í•œ í¬ê´„ì ì¸ ì—°êµ¬ ìˆ˜í–‰',
    backstory='ìµœì‹  AI ê¸°ìˆ  ë° íŠ¸ë Œë“œë¥¼ ë°íˆëŠ” ë° íŠ¹í™”ëœ ìˆ™ë ¨ëœ ì—°êµ¬ì›',
    verbose=True,
    allow_delegation=False,
    # LiteLLM ì‚¬ìš©ì„ ìœ„í•œ ì„¤ì •
    llm=f"openai/{MODEL_NAME}",
    api_base=API_BASE_URL,
    api_key=API_KEY,
    max_tokens=2000,
    temperature=0.7
)

writer = Agent(
    role='ì „ë¬¸ ì½˜í…ì¸  ì‘ê°€',
    goal='ì£¼ì–´ì§„ ì—°êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ ì‘ì„±',
    backstory='ë…ìë“¤ì—ê²Œ ë³µì¡í•œ ê°œë…ì„ ëª…í™•í•˜ê³  ë§¤ë ¥ì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ë° ë›°ì–´ë‚œ ì‘ê°€',
    verbose=True,
    allow_delegation=False,
    # ê°™ì€ LiteLLM ì„¤ì • ì‚¬ìš©
    llm=f"openai/{MODEL_NAME}",
    api_base=API_BASE_URL,
    api_key=API_KEY,
    max_tokens=2000,
    temperature=0.8
)

# 2. ì‘ì—… ì •ì˜
research_task = Task(
    description='''2025ë…„ ìµœì‹  AI íŠ¸ë Œë“œë¥¼ ì¡°ì‚¬í•˜ê³  ì£¼ìš” ë°œê²¬ ì‚¬í•­ì„ ìš”ì•½í•©ë‹ˆë‹¤. 
    ë‹¤ìŒ ì˜ì—­ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤:
    1. ìƒì„±í˜• AIì˜ ìµœì‹  ë°œì „ì‚¬í•­
    2. ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥
    3. AI ìœ¤ë¦¬ ë° ê·œì œ ë™í–¥
    4. ì‚°ì—…ë³„ AI ì ìš© ì‚¬ë¡€
    
    ê° ì˜ì—­ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í†µê³„ë¥¼ í¬í•¨í•˜ì„¸ìš”.''',
    expected_output='2025ë…„ AI íŠ¸ë Œë“œì— ëŒ€í•œ ì£¼ìš” í†µì°°ë ¥, í†µê³„ ë° ì‹¤ì œ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ëŠ” 300ë‹¨ì–´ ë¶„ëŸ‰ì˜ ìì„¸í•œ ì—°êµ¬ ìš”ì•½',
    agent=researcher
)

write_task = Task(
    description='''ì—°êµ¬ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ "2025ë…„ AI íŠ¸ë Œë“œ: ë¯¸ë˜ë¥¼ ë°”ê¾¸ëŠ” ê¸°ìˆ ë“¤"ì´ë¼ëŠ” ì œëª©ì˜ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ì‘ì„±í•©ë‹ˆë‹¤.
    
    ìš”êµ¬ì‚¬í•­:
    - 500-700ë‹¨ì–´ ë¶„ëŸ‰
    - ë§¤ë ¥ì ì¸ ë„ì…ë¶€ì™€ ê²°ë¡ 
    - ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
    - ë…ìì˜ ê´€ì‹¬ì„ ë„ëŠ” ë¶€ì œëª© í™œìš©
    - ì‹¤ì œ ì‚¬ë¡€ë‚˜ ì˜ˆì‹œ í¬í•¨
    
    ëŒ€ìƒ ë…ì: AIì— ê´€ì‹¬ìˆëŠ” ì¼ë°˜ ëŒ€ì¤‘''',
    expected_output='ë…ìì˜ ì°¸ì—¬ë¥¼ ìœ ë„í•˜ê³  ì •ë³´ê°€ í’ë¶€í•˜ë©° ì˜ êµ¬ì„±ëœ 500-700ë‹¨ì–´ ë¶„ëŸ‰ì˜ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼',
    agent=writer,
    context=[research_task]
)

# 3. í¬ë£¨ ì •ì˜ ë° ì‹¤í–‰
crew = Crew(
    agents=[researcher, writer],
    tasks=[research_task, write_task],
    process=Process.sequential,
    verbose=True,
    max_execution_time=600  # ìµœëŒ€ ì‹¤í–‰ ì‹œê°„ 10ë¶„
)

def run_crew_with_error_handling():
    """ì—ëŸ¬ ì²˜ë¦¬ê°€ í¬í•¨ëœ í¬ë£¨ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("=" * 50)
        print("í¬ë£¨ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("=" * 50)
        
        result = crew.kickoff()
        
        print("\n" + "=" * 50)
        print("## í¬ë£¨ ì‘ì—… ì™„ë£Œ ##")
        print("=" * 50 + "\n")
        print(result)
        
        return result
        
    except Exception as e:
        print(f"\nâŒ í¬ë£¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        
        # ì¼ë°˜ì ì¸ í•´ê²° ë°©ë²• ì œì‹œ
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. VLLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("2. ëª¨ë¸ ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”")
        print("3. API í‚¤ì™€ ì—”ë“œí¬ì¸íŠ¸ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("4. LiteLLM ë²„ì „ì„ í™•ì¸í•˜ì„¸ìš”: pip install --upgrade litellm")
        
        # ìƒì„¸ ë””ë²„ê¹… ì •ë³´
        print(f"\nğŸ” ë””ë²„ê¹… ì •ë³´:")
        print(f"ëª¨ë¸: {MODEL_NAME}")
        print(f"API Base: {API_BASE_URL}")
        print(f"API Key: {'ì„¤ì •ë¨' if API_KEY else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        
        return None

# ì‹¤í–‰
if __name__ == "__main__":
    result = run_crew_with_error_handling()
    
    if result:
        print("\nâœ… ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")